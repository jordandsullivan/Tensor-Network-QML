diff --git a/Potential QML Paper Contributions.docx b/Potential QML Paper Contributions.docx
new file mode 100644
index 0000000..04d166c
Binary files /dev/null and b/Potential QML Paper Contributions.docx differ
diff --git a/README.md b/README.md
index b43a719..ae9e03b 100644
--- a/README.md
+++ b/README.md
@@ -1,4 +1,5 @@
 To train:
-python train.py --savefile <name for params file> --data <mnist/bw/grey> --method <spsa/pyswarm>
+python train.py --savefile <params_filename> --data <mnist/bw/grey> --method <spsa/pyswarm>
+
 To test:
-python test.py --file <params file> --data <mnist/bw/grey>
+python test.py --file <params_filename> --data <mnist/bw/grey>
diff --git a/data/.DS_Store b/data/.DS_Store
new file mode 100644
index 0000000..b6f9b04
Binary files /dev/null and b/data/.DS_Store differ
diff --git a/data/.ipynb_checkpoints/Untitled-checkpoint.ipynb b/data/.ipynb_checkpoints/Untitled-checkpoint.ipynb
new file mode 100644
index 0000000..2fd6442
--- /dev/null
+++ b/data/.ipynb_checkpoints/Untitled-checkpoint.ipynb
@@ -0,0 +1,6 @@
+{
+ "cells": [],
+ "metadata": {},
+ "nbformat": 4,
+ "nbformat_minor": 2
+}
diff --git a/evolve_training.py b/evolve_training.py
new file mode 100644
index 0000000..bf33619
--- /dev/null
+++ b/evolve_training.py
@@ -0,0 +1,36 @@
+import numpy as np
+import math
+from pyswarm import pso
+import utils
+from scipy.optimize import differential_evolution
+
+def train(train_data, train_labels, mod, hyperparams): #mutate=>[0,2], recombine=>[0,1]
+    """
+    """
+
+    # Set hyperparameters:
+    lam, eta, batch_size = (hyperparams['lam'],hyperparams['eta'],hyperparams['batch_size'])
+
+    if 'mutate' in list(hyperparams.keys()):
+        mutate = hyperparams['mutate']
+    else:
+        mutate = 1.6960892059264037
+    if 'recombine' in list(hyperparams.keys()):
+        recombine = hyperparams['recombine']
+    else:
+        recombine = 0.7311832672620681
+    max_iter = 50
+    #n = len(train_data[0])
+    # print("n: %d" % n)
+
+    #Save parameters
+    params = list(2*math.pi*np.random.rand(mod.count))
+    dim = len(params)
+    print("Number of parameters in circuit: %d " % dim)
+
+    bounds = (np.zeros(dim), 2*math.pi*np.ones(dim))
+
+    xopt, fopt = differential_evolution(mod.get_loss, tuple(zip(bounds[0],bounds[1])), args=(train_data, train_labels, lam, eta, batch_size), maxiter = max_iter, mutation=mutate, recombination=recombine, tol= 0.000002)
+    print(xopt, fopt)
+    params = xopt
+    return params, mod
diff --git a/params/SPSA.csv b/params/SPSA.csv
new file mode 100644
index 0000000..66fa098
--- /dev/null
+++ b/params/SPSA.csv
@@ -0,0 +1,273 @@
+5.916700090428148151e+00
+5.277555594976061748e+00
+5.747678824649142371e+00
+3.716047710693013872e+00
+3.475723556224450839e+00
+5.340242349658089971e+00
+3.933722804815367180e+00
+1.001254161619034244e+00
+5.705443026301952436e+00
+2.657003813736227293e+00
+2.994971680362517041e+00
+6.024354193228615095e+00
+2.127778159266194624e+00
+1.922737796220842954e+00
+3.905951374463702219e+00
+6.013722798561066618e+00
+1.809389278400887369e+00
+5.148843590736650988e+00
+5.692680296977534304e+00
+1.394602796095674835e+00
+2.894250214023504242e+00
+1.092422872617883733e+00
+2.294775785646105959e+00
+3.473850418691137865e+00
+5.454473946402941031e+00
+1.932557895765930622e+00
+2.054280212724237664e+00
+3.841912762289742211e+00
+1.786926739391988539e+00
+3.011917971290494211e+00
+4.694123260861264413e+00
+4.340394700524401905e+00
+3.562185899405550416e+00
+5.895106695246108863e+00
+5.390469811084710017e+00
+5.859237946160243737e+00
+4.610728737759977464e-02
+4.530266034025028254e+00
+2.306605348109064924e+00
+5.197783891126231737e+00
+1.896252755961230552e+00
+2.726827232329945616e+00
+4.453379722453773226e+00
+2.300358521342557250e+00
+7.812512592174981618e-01
+4.240220850924607987e+00
+8.295849536799848600e-01
+1.821699600300709987e+00
+4.667863462425777321e+00
+4.261888062922121634e+00
+3.850840653289894444e+00
+5.174251538362358183e+00
+6.274302585951563316e-02
+5.609492759427154418e+00
+4.503966622443821066e+00
+5.331689860139980919e-01
+3.744559318297831840e+00
+2.268093155485066514e+00
+4.280072629298128817e+00
+2.116132062215579257e+00
+3.909462789733359589e+00
+2.554188642558046229e+00
+3.472908479553939110e+00
+6.776351734835202312e-02
+8.801378692663964056e-04
+5.942758776352548900e+00
+4.619122135280504970e+00
+3.348980143089131944e-01
+2.529705733924125344e+00
+4.974788129799630476e-01
+6.006576772991012447e+00
+1.350798783871357767e+00
+2.243194331878309189e+00
+2.618914956245659731e+00
+1.930883233261702525e+00
+1.448859970433132860e+00
+3.228945650894210484e+00
+5.012432376538272116e+00
+3.981824833037996392e+00
+1.264385934346206053e+00
+3.876709355829419668e+00
+1.060261941594428770e+00
+7.303266097918682531e-01
+2.037984351797887062e+00
+1.231183044875989507e+00
+5.492811431126328081e+00
+6.154159704856330126e-01
+5.628527967310517077e+00
+6.029026256838487807e-01
+2.106875572619117065e+00
+1.259742303762283022e+00
+1.782760764189337710e+00
+5.115584067844041627e+00
+4.907776182125219933e+00
+3.243237954687892444e+00
+1.108268606099325426e+00
+6.105793696132656656e+00
+1.232259680134542901e+00
+3.549768968481482823e+00
+1.525757320679533846e+00
+5.256699608184139727e+00
+5.452973471085515556e+00
+1.606458109979068405e-01
+3.341396864165247838e+00
+2.801335413876691494e+00
+1.777259929992979215e+00
+5.440506881302270337e+00
+1.016901664545605932e+00
+5.433112396739430139e+00
+4.647977168552980487e+00
+4.358503856394294829e+00
+2.347930422530438044e+00
+2.664988625868550898e+00
+1.564655527630101517e+00
+2.922330773377789903e+00
+3.481225742231810916e+00
+1.874861787750154551e+00
+4.998407316367177167e+00
+4.328996546592711603e+00
+4.548249960758170118e+00
+1.217905775393928636e+00
+1.508650333639305785e+00
+2.533278320325685673e+00
+5.909783409416993649e+00
+2.243083264784599518e+00
+1.943443394127869484e+00
+5.816093153532406745e+00
+3.494287357802444038e+00
+1.105732725711897402e+00
+5.109108483995244754e+00
+5.809289039109475361e+00
+5.727925065897513157e+00
+5.581485345513219620e+00
+5.946444905209999021e+00
+2.521286485375844766e+00
+4.095708032703309520e+00
+4.094705452494672215e+00
+6.187138474088873075e+00
+6.054315757908036666e+00
+4.110356044659948083e+00
+3.634572365189801868e+00
+9.349218342849552332e-01
+3.035306433358570199e+00
+4.759892834576545440e+00
+1.533795374741981243e+00
+4.306368323576414525e+00
+6.902134889678803731e-01
+8.977464100509728873e-01
+4.850101870553460870e+00
+6.832771785277458321e-02
+4.493921406766610005e+00
+3.313767999831168876e+00
+4.980066850863344108e+00
+3.208251634145256492e+00
+7.619942472537143408e-01
+4.266943324361856860e+00
+4.797036173164500816e+00
+1.427028946218233951e+00
+5.652601461463438781e+00
+2.945722933615931183e+00
+2.049888504969326952e+00
+3.616103499380749486e+00
+4.208561246041891302e+00
+5.380029297393542187e+00
+4.392687828509253656e+00
+1.253660607004905003e+00
+4.224933914104183508e+00
+2.152207434283379683e+00
+1.045319058038814530e+00
+2.682738235611901612e+00
+2.007643839262220453e+00
+4.309245482004120831e+00
+2.573036278488405326e+00
+3.902879216251297745e+00
+4.377172850016075145e+00
+2.323551810088211322e+00
+3.990463813896856227e+00
+6.175277650582051692e+00
+1.875971003885305022e+00
+5.439966134631051276e+00
+2.763003859931209227e+00
+2.943684231570361298e+00
+3.609508585908885792e+00
+3.149151052751315127e+00
+3.852652586937538981e+00
+3.843620710850333744e+00
+5.202857529394079528e+00
+4.543255771182140634e+00
+3.782078126206055213e+00
+5.855319652316337553e+00
+3.761631931498387704e+00
+3.768029837908355439e+00
+4.956849970878996814e+00
+3.087763326634516581e+00
+1.123512596700356747e-01
+3.835192132777974994e+00
+5.504357972254632969e+00
+8.262414972562064364e-01
+2.824630336392634611e+00
+6.002199264167467518e+00
+4.125858186514566484e+00
+1.470327311861993502e+00
+3.211050344626979136e+00
+5.554182179560401167e+00
+4.034596597074320279e+00
+1.385337214992933941e+00
+5.239861653174598288e+00
+2.903340774558499371e-01
+3.767624344208237552e+00
+4.682426944307167993e+00
+5.936351182747091393e+00
+4.688609523156015868e+00
+7.819924710493454667e-02
+5.558649504859040080e-01
+3.673511301099885440e+00
+1.983143437300546985e+00
+1.346418058066545020e+00
+1.182903557153975216e+00
+2.770470921157429656e-01
+2.667896653649795446e+00
+5.070812620495871670e+00
+2.273909325005324611e+00
+5.657530324861330229e+00
+5.139914877837552076e+00
+3.523783630584284232e+00
+2.832714424922407659e+00
+2.914689266199614881e+00
+2.844662351869784001e+00
+4.867221210317836722e+00
+1.412908447492347230e+00
+1.885567133127461492e+00
+4.455681173156770258e-01
+4.799127239736453165e-01
+5.157045575416963068e+00
+6.061970111341652867e+00
+1.817840334015023984e+00
+3.554273534938667378e+00
+5.972882198445026880e+00
+5.489591693732689848e+00
+4.683580682148844154e+00
+3.199929292085221810e+00
+6.159834982286348293e+00
+4.172550984909986771e-01
+5.585893346003460991e-01
+4.401356963720209237e+00
+3.877380232930905724e+00
+1.813374038794105525e+00
+1.938408358770131956e+00
+3.309448180669154027e+00
+5.292646443896536823e+00
+4.507806337129911967e+00
+2.780807381738453099e+00
+3.542680091598153780e+00
+3.541160936955158878e+00
+1.057285170595956947e+00
+4.764554497921209553e+00
+1.613418564606770689e+00
+5.879214620158760063e+00
+2.068387155398715649e+00
+2.777981177801505108e+00
+4.448518215919365915e+00
+2.298055618661761113e+00
+2.033145218525092890e+00
+2.253850059198998945e+00
+8.215193662149585796e-01
+2.366616693085369150e+00
+5.085221118099796378e+00
+3.193150357379165083e+00
+4.782231053267036458e+00
+4.267917140605884896e+00
+4.741773630503159254e+00
+4.032276300129303825e-01
+9.872315569119920475e-01
diff --git a/params/TRAIN.csv b/params/TRAIN.csv
new file mode 100644
index 0000000..fa75aaf
--- /dev/null
+++ b/params/TRAIN.csv
@@ -0,0 +1,273 @@
+4.750850934974188533e+00
+2.751023296691359032e+00
+1.136406611859092752e+00
+2.813143439159322856e+00
+1.547489634205857723e+00
+1.031658374867214567e+00
+4.824635499180762288e+00
+4.006383070840941585e+00
+2.960328892422651847e+00
+3.726888502014509719e+00
+2.548439507572899831e+00
+3.484657583214942900e+00
+4.502443892258866320e+00
+5.246266745496650508e+00
+1.936236411544519864e+00
+4.900024694006302362e+00
+2.872812691676355445e+00
+2.739108303597878269e+00
+6.572042719773621977e-01
+5.016218600793087568e+00
+2.256879674983595319e+00
+5.723751350764371360e+00
+1.505468239573886091e+00
+4.798714854728684642e+00
+3.337443551874407000e+00
+5.834246773264546881e+00
+2.992933458099229505e+00
+3.289991776505284182e+00
+4.557821443167953568e+00
+2.827912184229445458e+00
+2.311575741042884324e+00
+5.176405878620730761e+00
+2.141766516838340184e+00
+3.902945869398865231e+00
+4.835815705840393441e+00
+5.971533428870768878e-01
+2.162414352186413868e+00
+4.114982903213341814e+00
+3.983354085592769156e+00
+1.856505737594091654e+00
+2.233481970669452199e+00
+9.200229254633829346e-02
+5.191168347744598677e+00
+1.548127115603372683e+00
+5.892194803525659275e+00
+2.008531591911228154e-01
+9.888804503363864962e-01
+5.958325452510057296e+00
+5.233938697380703609e+00
+5.620034733513284309e+00
+9.397793228510965546e-01
+4.663207202070054791e+00
+3.238811095326387779e+00
+8.122471087290163982e-01
+7.581622358307207321e-01
+5.913848859228903798e+00
+5.616242170285840274e+00
+3.972898069647288199e+00
+2.216321319073486951e+00
+8.760627506186882130e-01
+4.569058769464509950e+00
+5.815167078993113137e+00
+1.413634367156075600e+00
+3.214086866164261647e+00
+5.067582792254443191e+00
+1.854780870297423556e+00
+1.474713125325608054e+00
+3.741838194825852226e-01
+5.096534106941725817e+00
+2.501112185032990265e+00
+5.276502173284295161e+00
+1.957380949807953296e+00
+2.717995254791734538e+00
+1.424041422584256145e+00
+6.026660788325512996e+00
+2.211846616079768513e+00
+2.689054939545314937e+00
+1.853279545506462789e+00
+1.741193117656015144e+00
+3.175011651975869764e+00
+2.399385026933955611e+00
+2.202973360518591495e+00
+3.543138900454481099e+00
+4.827549753979689662e+00
+4.328372068342763335e+00
+5.009475155665923829e+00
+6.174060095086814748e+00
+5.619965286114628533e+00
+2.274389929393581422e+00
+5.324195478586794650e+00
+4.985535031341538748e+00
+2.420180335356158507e-01
+6.667853591837310523e-01
+1.111782250684843598e+00
+5.544979822152260596e+00
+2.077171566535757297e+00
+2.055839437179408868e+00
+2.534877643357079102e-01
+3.487108167759571664e+00
+5.114163768066737781e+00
+4.520344784945531380e+00
+3.188157286895693776e+00
+1.847029309965188659e+00
+2.431528734661063229e+00
+2.412774634876006186e+00
+5.308235011916782042e+00
+1.447967137094690715e+00
+3.113670649690605696e+00
+4.934539052860599462e+00
+4.541182852015521298e+00
+2.918388784992272944e-01
+3.929573407820041719e+00
+6.105327396636992532e+00
+2.419419365511230602e+00
+3.866984340303374790e+00
+1.871614603284625034e+00
+6.069545549189733435e+00
+4.920908999141413886e+00
+2.456353578520825032e+00
+4.121636961902883201e+00
+2.022928564297817644e+00
+4.169783759916376731e+00
+2.259453114572234078e+00
+2.415859787399936565e+00
+5.707725940119369845e+00
+3.259358285899579322e+00
+1.171663012755297162e+00
+3.566039182574856636e-01
+4.686943329048236961e+00
+1.994593319169124523e+00
+3.058136593204739739e+00
+2.425227416324511776e+00
+4.884656783513179334e+00
+2.297004230451744622e+00
+6.111489329691004890e+00
+6.030389390478334555e+00
+2.427616975830599877e+00
+4.000032850429136921e+00
+6.008515524480169567e+00
+3.777378762714753702e+00
+5.720580550817343735e+00
+3.183714511667544489e+00
+2.020910349479493995e+00
+4.305122448738931240e+00
+6.137573274544456225e+00
+3.434173932927725215e+00
+7.298206115056758980e-01
+6.195354235066933946e+00
+4.367228097137746623e+00
+5.575811754997625158e+00
+2.436185550089198681e+00
+1.869672124100764998e+00
+2.443095564443874268e-01
+3.047621282705633927e+00
+5.482185890623010316e+00
+2.187661661972912341e+00
+1.419001229079596582e+00
+4.942216675296598360e+00
+6.074969885671413827e+00
+2.949415244979869843e+00
+5.937989904410542863e+00
+4.421244349672453389e+00
+9.500059862990170023e-01
+2.766521201666195307e+00
+2.927585152306978244e+00
+7.380380024408522699e-02
+2.124608369699391819e+00
+3.501187008066083961e-01
+5.595396235722852829e+00
+2.416646950408932071e+00
+2.675828047042394520e+00
+6.597908048628983169e-01
+4.757367357624233684e+00
+2.100609682879201578e+00
+4.350735698751566072e-01
+1.063012440440150197e+00
+6.157865705026522862e-01
+5.340959420191984108e+00
+3.384329633303973517e-01
+2.289434596292508672e+00
+1.303042884754531117e-01
+4.383990724640950631e+00
+5.481037911844913069e+00
+-1.363393030798406950e-02
+5.233669577141198737e+00
+4.852043039868964236e+00
+4.731704276917003682e+00
+5.387865091692321862e+00
+4.436156088440995071e-01
+3.248638318863584229e-01
+1.788943803271209498e+00
+7.733544290888456629e-01
+5.702749154404609833e+00
+1.654960322874830192e+00
+6.056499874253677973e-01
+4.553997430649208589e+00
+8.264916760197221768e-01
+3.210887704074564475e+00
+2.711145276279133043e+00
+2.801895465539993246e+00
+1.037897355253359771e-01
+4.594503858039046484e+00
+6.118242797287726020e+00
+1.404255255189167828e+00
+3.308093596377781775e+00
+1.420555948359160547e+00
+2.329475951306621884e+00
+2.117636282762099587e+00
+8.629415205748969253e-02
+5.143906901915434204e+00
+2.177439984693571251e+00
+1.338016912975637629e+00
+4.771429529441189032e+00
+5.069943708812568950e+00
+3.084830944637464434e+00
+4.972060945158119694e+00
+6.120961734503951313e+00
+5.748487664362708749e+00
+6.178169349718149128e+00
+5.035252542830630063e+00
+2.142266664693618594e+00
+5.646886754965738220e+00
+2.806326771047189972e+00
+2.542346777479519204e+00
+2.115869425904668422e+00
+1.486841440572660433e+00
+2.732353371771836681e+00
+4.300339305737285400e+00
+5.273426370573761623e+00
+5.263048345140892259e+00
+5.017284229195722034e+00
+6.243755652681184820e+00
+5.219839253930121714e+00
+4.257398691043551509e+00
+1.691809623118030625e+00
+4.924583522928170254e+00
+4.751099590371335779e+00
+4.254910789917558667e+00
+1.560211160296079314e+00
+6.308423339326948209e+00
+5.377303743187151319e+00
+3.076128535219070592e+00
+3.161562842299773024e+00
+5.750609681890091807e+00
+3.121861775646122439e+00
+2.665641078871149983e+00
+4.323195337339439948e-01
+2.698054968402078035e+00
+6.199155323740539458e+00
+5.515462094750236410e+00
+5.443075231078031706e+00
+3.410174192060232645e+00
+8.543946173057609395e-01
+3.511578783932743697e+00
+5.892570874612759191e+00
+5.517746071311457001e+00
+5.111936263070789188e+00
+4.330024557459083212e+00
+4.054843655222650334e+00
+5.828327772175739163e+00
+1.491574342639405248e+00
+3.313396921791845084e+00
+2.986443818842576259e-01
+2.277842443108687664e+00
+5.393679918773955251e+00
+2.548218494423508673e+00
+2.620971603178789966e+00
+2.357427429273734276e+00
+2.319579475024641901e+00
+4.764271304854790223e+00
+1.645306222359692816e+00
+4.891998995564129232e-02
+1.662486679622250163e+00
diff --git a/pyswarm_training.py b/pyswarm_training.py
index a25a276..f6effb4 100644
--- a/pyswarm_training.py
+++ b/pyswarm_training.py
@@ -4,18 +4,19 @@ from pyswarm import pso
 import utils
 
 
-def train(train_data, train_labels, mod):
+def train(train_data, train_labels, mod, hyperparams):
     """
     """
 
     # Set hyperparameters:
-    num_epochs = 20
-    swarm_size = 10
-    batch_size = 10
-    lam = .33
-    eta = 1.0
+    lam, eta, batch_size = (hyperparams['lam'],hyperparams['eta'],hyperparams['batch_size'])
 
+    if 'swarm_size' in list(hyperparams.keys()):
+        swarm_size =  hyperparams['swarm_size']
+    else:
+        swarm_size = 70
 
+    num_epochs = 20
     n = len(train_data[0])
     # print("n: %d" % n)
 
diff --git a/sigopt_training.py b/sigopt_training.py
new file mode 100644
index 0000000..ed82dc7
--- /dev/null
+++ b/sigopt_training.py
@@ -0,0 +1,74 @@
+import pyswarm_training
+import spsa_training
+import evolve_training
+import sigopt_training
+
+import numpy as np
+import math
+import utils
+from sigopt import Connection
+
+
+conn = Connection(client_token="EUXXAFUFQRXKISDCAACDHOGJSOXOBZQRUCDSQTYRBXINEDJF")
+
+def train(train_data, train_labels, mod, method, hyperparams):
+    """
+    """
+    #Add model hyperparameters to be optimized
+    if method =='spsa':
+        hyperparams.update({'a':0, 'c':0})
+    elif method =='pyswarm':
+        hyperparams.update({'swarm_size':0})
+    elif method == 'evolve':
+        hyperparams.update({'mutate':0, 'recombine':0})
+
+    keys = hyperparams.keys()
+    #Define model hyperparameter ranges
+    lam_range = [0,1]
+    eta_range = [0,5]
+    batch_size_range = [10,100]
+
+    ###Define method hyperparamater ranges
+    #SPSA
+    a_range = [0,1]
+    c_range = [0,1]
+
+    #Evolutionary:
+    mutate_range = [0,2]
+    recombine_range = [0,1]
+
+    #Particle Swarm
+    swarm_size_range = [50,100]
+
+    #Define overall parameters for sigopt
+    parameters = []
+    for i in keys:
+        if i == 'batch_size' or i == 'swarm_size':
+            parameters.append(dict(name=i, type='int', bounds=dict(min=eval(i+'_range')[0], max=eval(i+'_range')[1])))
+        else:
+            parameters.append(dict(name=i, type='double', bounds=dict(min=eval(i+'_range')[0], max=eval(i+'_range')[1])))
+
+    #Create sigopt experiment
+    print(parameters)
+    experiment = conn.experiments().create(
+    name='QML Tensor Networks',
+    parameters=parameters,)
+    print("Created experiment: https://app.sigopt.com/experiment/" + experiment.id)
+
+    # Evaluate your model with the suggested parameter assignments
+
+    def evaluate_model(assignments):
+        return eval(method+'_training').train(train_data, train_labels, mod, assignments)
+
+    # Run the Optimization Loop between 10x - 20x the number of parameters
+    for _ in range(30):
+        suggestion = conn.experiments(experiment.id).suggestions().create()
+        value = evaluate_model(suggestion.assignments)
+        conn.experiments(experiment.id).observations().create(
+            suggestion=suggestion.id,
+            value=value,
+        )
+
+    hyperparams = suggestion.assignments
+    np.savetxt("hyper_params/" + args.savefile, hyperparams, delimiter = ",")
+    return hyperparams, mod
diff --git a/spsa_training.py b/spsa_training.py
new file mode 100644
index 0000000..346e79b
--- /dev/null
+++ b/spsa_training.py
@@ -0,0 +1,46 @@
+import numpy as np
+import math
+from noisyopt import minimizeSPSA
+
+
+import numpy as np
+import utils
+
+def train(train_data, train_labels, mod, hyperparams):
+    """
+    """
+
+    # Set hyperparameters:
+    lam, eta, batch_size, = (hyperparams['lam'],hyperparams['eta'],hyperparams['batch_size'])
+
+    if 'a' in list(hyperparams.keys()):
+        a = hyperparams['a']
+    else:
+        a = .03
+
+    if 'c' in list(hyperparams.keys()):
+        c = hyperparams['c']
+    else:
+        c = .1
+    num_iterations = 40 #100
+
+
+
+    n = len(train_data[0])
+    print("n: %d" % n)
+
+    #Save parameters
+    params = list(2*math.pi*np.random.rand(mod.count))
+    dim = len(params)
+    print("Number of parameters in circuit: %d " % dim)
+
+    bounds = (np.zeros(dim), 2*math.pi*np.ones(dim))
+
+    args = (train_data, train_labels, lam, eta, batch_size)
+    result = minimizeSPSA(mod.get_loss, args=args, x0=params, paired=True, niter=num_iterations, a=a, c=c)#, disp=True)#, bound=bounds)
+
+#    xopt, fopt = pso(model.get_loss, bounds[0], bounds[1], args=(n, train_data, train_labels, lam, eta, batch_size), swarmsize=swarm_size, maxiter=num_epochs)'
+    print(result)
+    params = result.x
+    print("number of training circuit runs = ", mod.num_runs)
+    return params, mod
diff --git a/test.py b/test.py
index 8e6242c..5b4a119 100644
--- a/test.py
+++ b/test.py
@@ -1,7 +1,11 @@
 import numpy as np
 import math
+<<<<<<< HEAD
+import model_simulate_wf as model
+=======
 # import model_full_simulate_wf as model
 import model
+>>>>>>> 70109a67a83384d027c32048db0128291d730729
 import utils
 import argparse
 from tqdm import tqdm
diff --git a/train.py b/train.py
index 2c426a6..4086659 100644
--- a/train.py
+++ b/train.py
@@ -1,39 +1,45 @@
 import pyswarm_training
-import noisyopt_training
-import spsa
+import spsa_training
+import evolve_training
+import sigopt_training
+
 import test
 import argparse
 import utils
 import numpy as np
-# import model_full_simulate_wf as model
-import model
+import model_simulate_wf as model
+import time
+
+start = time.time()
 
 parser = argparse.ArgumentParser(description='Train model')
 parser.add_argument("--savefile", help="filename to save params", type=str)
 parser.add_argument("--resumefile", help="filename to resume params from", type=str)
 # parser.add_argument("--data", help="bw, grey, or mnist", type = str, default = 'mnist')
 parser.add_argument("--method", help="pyswarm or spsa", type = str, default = 'spsa')
+parser.add_argument("--sigopt", help="optimze hyperparameters: True or False", type = bool, default = False)
 args = parser.parse_args()
-classes = (0,1)
-train_data, train_labels, test_data, test_labels = utils.load_mnist_data('data/4', classes)
-#TODO: make hyperparameters also arguments to train()
+
+train_data, train_labels, test_data, test_labels = utils.load_data('mnist')
+hyperparams = dict({'lam':0.023953588780914498, 'eta':4.465676125689359, 'batch_size':18})
+
 params = None
-n = 16#len(train_data[0])
-mod = model.Model(n=n, num_trials=1, classes=classes)
-# print(np.linalg.norm(train_data[0]), train_data[0], "hello")
-init_params= None
-init_params = np.random.normal(loc=0.0, scale=1.0, size=mod.count)
-test.get_accuracy(test_data[:200], test_labels[:200], mod, init_params)
-if args.resumefile:
-    init_params = utils.load_params(args.resumefile)
-if args.method == 'spsa':
-    params, mod = spsa.train(train_data, train_labels, mod, params=init_params)
+n = len(train_data[0])
+mod = model.Model(n=n, num_trials=1)
+
+if args.sigopt:
+    hyperparams, mod = sigopt_training.train(train_data, train_labels, mod, args.method, hyperparams)
+elif args.method == 'spsa':
+    params, mod = spsa_training.train(train_data, train_labels, mod, hyperparams)
 elif args.method == 'pyswarm':
-    params, mod = pyswarm_training.train(train_data, train_labels, mod)
+    params, mod = pyswarm_training.train(train_data, train_labels, mod, hyperparams)
+elif args.method == 'evolve':
+    params, mod = evolve_training.train(train_data, train_labels, mod, hyperparams)
 else:
     print("invalid optimization method")
     exit(0)
 
-
-np.savetxt("params/" + args.savefile, params, delimiter = ",")
-test.get_accuracy(test_data[:200], test_labels[:200], mod, params)
+if params is not None:
+    np.savetxt("params/" + args.savefile, params, delimiter = ",")
+test.get_accuracy(test_data, test_labels, mod, params)
+print('Total training time:',(time.time()-start)/60,'minutes')
